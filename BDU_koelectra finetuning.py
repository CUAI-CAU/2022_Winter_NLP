# -*- coding: utf-8 -*-
"""KoElectra_finetuned2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hk8inEciJWeX6TUXNwCYCXn0kRhxs7iz
"""

from google.colab import drive
drive.mount('/content/drive')

#Huggingface의 Transformer 다운로드
!pip install git+https://github.com/huggingface/transformers

!pip install tensorboardX

import math
import re
from random import *
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import matplotlib.pyplot as plt

from tensorboardX import SummaryWriter
from transformers import ElectraForMaskedLM, ElectraTokenizer

"""> 하이퍼파라미터 설정"""

BATCH_SIZE = 8
EPOCH = 100
LR = 0.001
MAX_LENGTH = 50
MODEL_NAME = "KoElectra_Generator3.pt"
DATA_NAME = 'CUAI_trainset_MLM.txt'
PATH = '/content/drive/MyDrive/nlpbook/'

"""#### 데이터 전처리
1.   Electra tokenizer로 토큰화 & Encoding(정수 인코딩)
2.   Tokenized list에서 '##'으로 시작하는 토큰들의 위치를 리스트 저장(mask_list)
3.   Electra tokenizer로 정수 인코딩 된 input_ids, token_type_ids, attention_mask의 각 위치에 [MASK]토큰에 해당하는 정수 4로 교체

"""

#Tokenizer 정의
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-generator")

"""##### DataLoader, dataset"""

def masking_sent(sent):
    token_sent = tokenizer.tokenize(sent)
    p = re.compile('^#')
    for idx, word in enumerate(token_sent):
        if p.match(word) != None:
            token_sent[idx] = '[MASK]'
    new_sent = ' '.join(token_sent)
    return new_sent

#사용자가 정의한 CustomDataset을 만들어줌
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, data_root):
        self.train_set = pd.read_csv(data_root, sep = '\n', engine= 'python', encoding = 'utf-8', header = None) 

    def __len__(self):
        return len(self.train_set)

    def __getitem__(self, idx): #CustomDataset을 호출하고 DataLoader에 넣으면 idx에 따라 적절한 배치를 만들어 줍니다
        if torch.is_tensor(idx):
            idx = idx.tolist() 

        train_sent = self.train_set.values[idx, 0] #예시: '어제는 날씨가 추웠어'
        masked_sent = masking_sent(train_sent)
        inputs = tokenizer(masked_sent, return_tensors = 'pt', padding = 'max_length', max_length = MAX_LENGTH)

        #label은 tokenizer 출력 중에서 ['input_ids']만 사용합니다 
        labels = tokenizer(train_sent, return_tensors = 'pt', padding = 'max_length', max_length = MAX_LENGTH)['input_ids']
        labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

        inputs['input_ids'] = inputs['input_ids'].squeeze()
        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze()
        inputs['attention_mask'] = inputs['attention_mask'].squeeze()

        #label도 그에 맞게 squeeze
        labels = labels.squeeze()
        
        return inputs, labels

dataset = CustomDataset(data_root = PATH + DATA_NAME)
#train, validation, test 나누기

train_size = int(len(dataset) * 0.8)
val_size = int(train_size * 0.1)
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])

train_DL = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)
val_DL = torch.utils.data.DataLoader(val_dataset, batch_size = int(BATCH_SIZE/2), shuffle = True)
test_DL = torch.utils.data.DataLoader(test_dataset, batch_size = int(BATCH_SIZE/2), shuffle = True)

"""#### Train(Fine-tuning)"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

lri = []
train_lossi = []
val_lossi = []
stepi = []

model = ElectraForMaskedLM.from_pretrained("monologg/koelectra-base-v3-generator")
model = model.to(device)

torch.autograd.set_detect_anomaly(True) #학습의 오류를 탐지하는 코드입니다

#학습 내용 시각화(Tensorboard) 
writer = SummaryWriter()

optimizer = torch.optim.AdamW(model.parameters(), lr = LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.5, last_epoch = -1)

for i in range(1, EPOCH+1):
    #summary = SummaryWriter
    #LR = 0.005 if i < (EPOCH/2) else 0.001
    running_loss = 0.0
    running_val_loss = 0.0

    model.train()
    for x, y in train_DL:
        x = x.to(device)
        y = y.to(device)
        output = model(**x, labels = y)
        loss = output.loss
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        running_loss += loss.item()
    trainning_loss = running_loss / len(train_DL)
    writer.add_scalar('Loss', trainning_loss, i) #텐서보드에 train loss 입력

    with torch.no_grad():
        for x, y in val_DL:
            x = x.to(device)
            y = y.to(device)
            output = model(**x, labels = y)
            val_loss = output.loss
            running_val_loss += val_loss.item()

    validation_loss = running_val_loss / len(val_DL)
    writer.add_scalar('validation loss', validation_loss, i)
    writer.add_scalar('Learning rate', optimizer.param_groups[0]['lr'], i )

    scheduler.step()
    train_lossi.append(trainning_loss)
    val_lossi.append(validation_loss)
    stepi.append(i)
    if i % 10 == 0:
        print(f"{i}/{EPOCH+1} : trainning_loss:{trainning_loss:.4f}, validation_loss:{validation_loss:.4f}")
        print()

# Commented out IPython magic to ensure Python compatibility.
#텐서보드 열기
# %load_ext tensorboard
# %tensorboard --logdir runs --port=6006

# if __name__ == '__main__':
#     with torch.no_grad():
#         output = model(**input.to(device), labels = label.to(device)) #output을 출력 
            #Hugginface의 Transformer, BERT 기반 모델의 출력은 
                #loss, logits, hidden_states, attentions 등을 가지고 있습니다.
            #따라서 따로 loss를 계산하지 않고, output.loss를 통해 바로 확인 가능합니다.
    #output.loss.item() #test로 찍어본 loss
    #output.loss.requires_grad #with torch.no_grad(): 로 했기 때문에 requires_grad = False

"""##### 모델 저장"""

def Save_LM_model(model, path):
    torch.save(model.state_dict(), path)

Save_LM_model(model = model, path = PATH + MODEL_NAME)

"""##### 모델 불러오기"""

#모델 불러오기
infer_Model =  ElectraForMaskedLM.from_pretrained("monologg/koelectra-base-v3-generator")
infer_Model.load_state_dict(torch.load('/content/drive/MyDrive/nlpbook/KoElectra_Generator.pt'))

"""#### 추론"""

#Tokenizer 정의
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-generator")

#hanspell 다운
!pip install git+https://github.com/ssut/py-hanspell.git

from hanspell import spell_checker

#모델이 정의되어 있어야 합니다
def sent_generation(sent): #sent는 list형식이 아니고 그냥 str ~ '나 배 고프 '
                                                #이렇게 문장 마지막에 띄어쓰기(공백)가 하나 있으면 좋습니다.
                                                #공백이 없다면 [MASK]를 넣지 않습니다.
    input_sent = " [MASK] ".join(sent.split(" "))
    inputs = tokenizer(input_sent, return_tensors = 'pt')
    with torch.no_grad(): #requires_grad = False
        model.eval()
        logits = model(**inputs).logits 
        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]   
    
        predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)

        token_list = []
        for token in predicted_token_id.tolist():
            token_list.append(tokenizer.decode(token))

        sentence = input_sent.split(" ")
        re_token_lst = []
        for token in token_list:
            re_token = re.sub("[^가-힣]", "", token)
            re_token_lst.append(re_token)
        i = 0
        for idx, word in enumerate(sentence): 
            if word == "[MASK]":
                sentence[idx] = re_token_lst[i] 
                i += 1 
        output = "".join(sentence)  

        spell_checked_output = spell_checker.check(output).as_dict()
        return spell_checked_output['checked']

if __name__ == '__main__':
    ex_sent = '나 배 고프 '
    input_sents = "[MASK] ".join(ex_sent.split(" "))
    inputs = tokenizer(input_sents, return_tensors = 'pt')
    print(input_sents)
    print(inputs)
    with torch.no_grad():
        infer_Model.eval()
        logits = infer_Model(**inputs).logits 
        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]   
    #print(mask_token_index)

    mask_token = []
    for idx in mask_token_index:
        predicted_token_id = logits[0, idx].argmax(axis=-1)
        decoded_token = tokenizer.decode(predicted_token_id) #decode된 token입니다 => 예시: ##는
        token = re.sub("[^가-힣]", "", decoded_token) # 정규표현식으로 글자가 아닌 것을 제거 => 예시: ##는 -> 는
        mask_token.append(token)
        print(predicted_token_id)
    print(mask_token)
    i = 0
    sentence = input_sents.split(" ") #output_sent는 ['나', '[MASK]', '배', '[MASK]', '고프', '[MASK]', ' ']
    print(sentence)
    # for idx, word in enumerate(sentence): 
        
    #     if word == "[MASK]":
    #         sentence[idx] = mask_token[i] #mask_token의 값으로 바꿔줍니다
    #         i += 1 #바꿔졌으면 mask_token의 다음 토큰으로 넘어갑니다
    # output = "".join(sentence) #예시: '나는배가고프다' 
    # spell_checked_output = spell_checker.check(output).as_dict() #str형태로 넘기기
    # print(spell_checked_output['checked'])